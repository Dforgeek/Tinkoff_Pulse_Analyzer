{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pymorphy2 as pymorphy2\n",
    "import seaborn as sns\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"posts.xlsx\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].fillna(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def standardize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    text = re.sub('_', ' ', text)\n",
    "    text = re.sub('http[^\\s]+', '', text)\n",
    "    text = re.sub('\\$\\d+\\.\\d{2}', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(\"[^\\w\\s]\",\"\", text) # возможно плохая\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\xa0', ' ', text)\n",
    "    text= re.sub(' +',' ', text)\n",
    "    return text\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['text'][8]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(standardize_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['text'][8]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Tokenization with NLTK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['token'] = df['text'].apply(word_tokenize)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_words = [word for tokens in df[\"token\"] for word in tokens]\n",
    "text_lengths = [len(tokens) for tokens in df[\"token\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(text_lengths))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Count')\n",
    "plt.hist(text_lengths, bins=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove stop words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_stopwords(text, vocab_set):\n",
    "    words = [w for w in text if w not in vocab_set]\n",
    "    return words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "df['token'] = df['token'].apply(lambda x: remove_stopwords(x, stopwords.words('russian')))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['token']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's look how many words left after removing stop words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_words_tr = [word for tokens in df[\"token\"] for word in tokens]\n",
    "text_lengths_tr = [len(tokens) for tokens in df[\"token\"]]\n",
    "vocab_tr = sorted(list(set(all_words_tr)))\n",
    "\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words_tr), len(vocab_tr)))\n",
    "print(\"Max sentence length is %s\" % max(text_lengths_tr))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Count')\n",
    "plt.hist(text_lengths_tr, bins=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Word's dropped in total:\", len(all_words)-len(all_words_tr))\n",
    "print(\"Word's dropped from vocabulary:\", len(VOCAB)-len(vocab_tr))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lemmatization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from pymorphy2 import MorphAnalyzer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    text = [morph.normal_forms(x)[0] for x in text]\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series_lem = df['token'].apply(lemmatize)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check words frequency"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus = [word for i in series_lem for word in i]\n",
    "corpus[10:20]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "most_common = FreqDist(corpus).most_common(20)\n",
    "words, frequency = [], []\n",
    "for word, count in most_common:\n",
    "    words.append(word)\n",
    "    frequency.append(count)\n",
    "\n",
    "sns.barplot(x = frequency, y = words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FreqDist(corpus).most_common(50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see, that words like \"**который**\" and \"**это**\" are popular, but will not have impact on model. We should drop them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stop_words = [\"это\", \"который\", \"свой\", 'такой', 'также']\n",
    "series_lem = series_lem.apply(lambda x: remove_stopwords(x, stop_words))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus = [word for i in series_lem for word in i]\n",
    "FreqDist(corpus).most_common(100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "n=2\n",
    "ngrams_2_series = series_lem.apply(lambda x: list(map(' '.join, ngrams(x, n=n))))\n",
    "FreqDist([word for i in ngrams_2_series for word in i]).most_common(100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n=3\n",
    "ngrams_3_series = series_lem.apply(lambda x: list(map(' '.join, ngrams(x, n=n))))\n",
    "FreqDist([word for i in ngrams_3_series for word in i]).most_common(100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n=4\n",
    "ngrams_4_series = series_lem.apply(lambda x: list(map(' '.join, ngrams(x, n=n))))\n",
    "FreqDist([word for i in ngrams_4_series for word in i]).most_common(100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series_lem"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series_lem_sentences = [\" \".join(i) for i in series_lem]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000,\n",
    "                                 min_df=0.01,\n",
    "                                 use_idf=True, ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(series_lem_sentences)\n",
    "#vectorizer.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, k_means\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "db = DBSCAN(eps=1.18, min_samples=5).fit(X)\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_clust = df.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_clust['cluster'] = labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_clust[df_clust['cluster'] == 1].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_clust['post_link'] = \"https://www.tinkoff.ru/invest/social/profile/\" + df_clust['publisher'] + \"/\" + df_clust['id']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_clust[df_clust['cluster'] == 6].head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(-1, 8):\n",
    "    df_clust[df_clust['cluster'] == i]['post_link'].to_excel(f'./Clusters/cluster_{i}.xls')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
